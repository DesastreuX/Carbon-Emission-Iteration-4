{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:11:33.290238Z",
     "start_time": "2024-05-23T22:11:31.540674Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "from common.const import DATASET, FILEPATH, STAGING_FILENAME\n",
    "from common.utils import change_case\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import (DecisionTreeRegressor, GBTRegressor,\n",
    "                                   RandomForestRegressor)\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bfc18b9d93ca2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:11:40.838029Z",
     "start_time": "2024-05-23T22:11:33.292596Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/23 22:11:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "findspark.init(\"/home/ubuntu/spark-3.2.1-bin-hadoop2.7\")\n",
    "spark = SparkSession.builder.appName(\"basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f519d44882416b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:11:40.852953Z",
     "start_time": "2024-05-23T22:11:40.841544Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a550cd5592b220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:11:46.532200Z",
     "start_time": "2024-05-23T22:11:40.858845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dict = {\n",
    "    \"dt_imp_df\": spark.read.parquet(\n",
    "        f\"{FILEPATH.TEMP_STAGING_PATH}/{STAGING_FILENAME.DMA}_dt_importance.parquet\"\n",
    "    ),\n",
    "    \"gbt_imp_df\": spark.read.parquet(\n",
    "        f\"{FILEPATH.TEMP_STAGING_PATH}/{STAGING_FILENAME.DMA}_gbt_importance.parquet\"\n",
    "    ),\n",
    "    \"rf_imp_df\": spark.read.parquet(\n",
    "        f\"{FILEPATH.TEMP_STAGING_PATH}/{STAGING_FILENAME.DMA}_rf_importance.parquet\"\n",
    "    ),\n",
    "    \"03_df\": spark.read.parquet(\n",
    "        f\"{FILEPATH.TEMP_STAGING_PATH}/{STAGING_FILENAME.DP}.parquet\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625b54142e14964f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:27:46.880469Z",
     "start_time": "2024-05-23T22:11:46.534235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt_imp_df\n",
      "random_forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 22:14:14 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/23 22:14:14 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbt_imp_df\n",
      "random_forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 22:16:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 22:19:55 WARN BlockManager: Asked to remove block broadcast_12697, which does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 22:20:13 WARN BlockManager: Block rdd_19919_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_imp_df\n",
      "random_forest\n",
      "gradient_boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 22:22:22 WARN BlockManager: Block rdd_24694_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree\n",
      "03_df\n",
      "random_forest\n",
      "gradient_boost\n",
      "decision_tree\n"
     ]
    }
   ],
   "source": [
    "ran_for = RandomForestRegressor(featuresCol=\"features\")\n",
    "gbt = GBTRegressor(featuresCol=\"features\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\")\n",
    "ml_models = {\n",
    "    \"random_forest\": ran_for,\n",
    "    \"gradient_boost\": gbt,\n",
    "    \"decision_tree\": dt,\n",
    "}\n",
    "param_grid = {\n",
    "    \"random_forest\": ParamGridBuilder()\n",
    "    .addGrid(ran_for.maxDepth, [6, 7])\n",
    "    .addGrid(ran_for.maxBins, [16, 32])\n",
    "    .addGrid(ran_for.numTrees, [20, 30])\n",
    "    .addGrid(ran_for.subsamplingRate, [0.1, 1])\n",
    "    .build(),\n",
    "    \"gradient_boost\": ParamGridBuilder()\n",
    "    .addGrid(gbt.maxDepth, [6, 7])\n",
    "    .addGrid(gbt.maxBins, [16, 32])\n",
    "    .addGrid(gbt.maxIter, [20])\n",
    "    .build(),\n",
    "    \"decision_tree\": ParamGridBuilder()\n",
    "    .addGrid(dt.maxDepth, [6, 7])\n",
    "    .addGrid(dt.maxBins, [16, 32])\n",
    "    .build(),\n",
    "}\n",
    "best_models = {}\n",
    "for df_name, df in df_dict.items():\n",
    "    print(df_name)\n",
    "    feature_columns = df.columns.copy()\n",
    "    feature_columns.remove(change_case(DATASET.TARGET))\n",
    "    vec_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    vec_df = vec_assembler.transform(df)\n",
    "    vec_df = vec_df.withColumnRenamed(change_case(DATASET.TARGET), \"label\")\n",
    "    train, test = vec_df.randomSplit([0.8, 0.2])\n",
    "    best_models[df_name] = {}\n",
    "    for model_name, ml_model in ml_models.items():\n",
    "        print(model_name)\n",
    "        cv = CrossValidator(\n",
    "            estimator=ml_model,\n",
    "            estimatorParamMaps=param_grid[model_name],\n",
    "            evaluator=RegressionEvaluator(),\n",
    "            parallelism=2,\n",
    "            numFolds=3,\n",
    "        )\n",
    "        model = cv.fit(train)\n",
    "        best_models[df_name][model_name] = {\n",
    "            \"model\": model.bestModel,\n",
    "        }\n",
    "    for model_name, value in best_models[df_name].items():\n",
    "        y_pred = value[\"model\"].transform(test)\n",
    "        valuesAndPreds = y_pred.select([\"label\", \"prediction\"])\n",
    "        valuesAndPreds = valuesAndPreds.withColumn(\n",
    "            \"label\", col(\"label\").cast(DoubleType())\n",
    "        )\n",
    "        valuesAndPreds = valuesAndPreds.rdd.map(tuple)\n",
    "        metrics = RegressionMetrics(valuesAndPreds)\n",
    "        best_models[df_name][model_name][\"score\"] = {\n",
    "            \"r2\": metrics.r2,\n",
    "            \"mse\": metrics.meanSquaredError,\n",
    "            \"rmse\": metrics.rootMeanSquaredError,\n",
    "            \"mae\": metrics.meanAbsoluteError,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb8476b60ec2c11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:27:46.896474Z",
     "start_time": "2024-05-23T22:27:46.882160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt_imp_df\n",
      "\trandom_forest\n",
      "\t\tmodel: RandomForestRegressionModel: uid=RandomForestRegressor_bd7a78c895d0, numTrees=30, numFeatures=7\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.5166498879386539\n",
      "\t\t\tmse: 320017.85300358065\n",
      "\t\t\trmse: 565.7012047040209\n",
      "\t\t\tmae: 408.944532935262\n",
      "\tgradient_boost\n",
      "\t\tmodel: GBTRegressionModel: uid=GBTRegressor_a4ee322a2040, numTrees=20, numFeatures=7\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.5797021589551029\n",
      "\t\t\tmse: 348193.37101294973\n",
      "\t\t\trmse: 590.0791226716547\n",
      "\t\t\tmae: 417.1171920268038\n",
      "\tdecision_tree\n",
      "\t\tmodel: DecisionTreeRegressionModel: uid=DecisionTreeRegressor_0a946d3fbc42, depth=6, numNodes=127, numFeatures=7\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.4991844028890783\n",
      "\t\t\tmse: 372402.58319812926\n",
      "\t\t\trmse: 610.2479686145045\n",
      "\t\t\tmae: 437.5841602669964\n",
      "gbt_imp_df\n",
      "\trandom_forest\n",
      "\t\tmodel: RandomForestRegressionModel: uid=RandomForestRegressor_bd7a78c895d0, numTrees=20, numFeatures=15\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.4344631200650574\n",
      "\t\t\tmse: 351928.608907014\n",
      "\t\t\trmse: 593.2357110854117\n",
      "\t\t\tmae: 425.77634001048904\n",
      "\tgradient_boost\n",
      "\t\tmodel: GBTRegressionModel: uid=GBTRegressor_a4ee322a2040, numTrees=20, numFeatures=15\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.5900413198732593\n",
      "\t\t\tmse: 367842.791830654\n",
      "\t\t\trmse: 606.5004466862774\n",
      "\t\t\tmae: 420.3355841084723\n",
      "\tdecision_tree\n",
      "\t\tmodel: DecisionTreeRegressionModel: uid=DecisionTreeRegressor_0a946d3fbc42, depth=6, numNodes=127, numFeatures=15\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.4934579202273328\n",
      "\t\t\tmse: 392861.91636393854\n",
      "\t\t\trmse: 626.7869784575446\n",
      "\t\t\tmae: 447.4402611666784\n",
      "rf_imp_df\n",
      "\trandom_forest\n",
      "\t\tmodel: RandomForestRegressionModel: uid=RandomForestRegressor_bd7a78c895d0, numTrees=30, numFeatures=27\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.46546162560403004\n",
      "\t\t\tmse: 311054.97221684555\n",
      "\t\t\trmse: 557.7230246429185\n",
      "\t\t\tmae: 408.2016833340216\n",
      "\tgradient_boost\n",
      "\t\tmodel: GBTRegressionModel: uid=GBTRegressor_a4ee322a2040, numTrees=20, numFeatures=27\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.5954701423964608\n",
      "\t\t\tmse: 326306.2687928674\n",
      "\t\t\trmse: 571.2322371792994\n",
      "\t\t\tmae: 397.19465106173436\n",
      "\tdecision_tree\n",
      "\t\tmodel: DecisionTreeRegressionModel: uid=DecisionTreeRegressor_0a946d3fbc42, depth=6, numNodes=127, numFeatures=27\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.5008556513548554\n",
      "\t\t\tmse: 361938.42968050175\n",
      "\t\t\trmse: 601.6131894170055\n",
      "\t\t\tmae: 435.50804449958395\n",
      "03_df\n",
      "\trandom_forest\n",
      "\t\tmodel: RandomForestRegressionModel: uid=RandomForestRegressor_bd7a78c895d0, numTrees=30, numFeatures=28\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.47455173545568163\n",
      "\t\t\tmse: 309890.70455843315\n",
      "\t\t\trmse: 556.6782774264083\n",
      "\t\t\tmae: 403.5976808178366\n",
      "\tgradient_boost\n",
      "\t\tmodel: GBTRegressionModel: uid=GBTRegressor_a4ee322a2040, numTrees=20, numFeatures=28\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.602258180224287\n",
      "\t\t\tmse: 320042.4138631845\n",
      "\t\t\trmse: 565.7229126199367\n",
      "\t\t\tmae: 394.46748391602665\n",
      "\tdecision_tree\n",
      "\t\tmodel: DecisionTreeRegressionModel: uid=DecisionTreeRegressor_0a946d3fbc42, depth=6, numNodes=127, numFeatures=28\n",
      "\t\tscore:\n",
      "\t\t\tr2: 0.4923044025934603\n",
      "\t\t\tmse: 368919.85453263094\n",
      "\t\t\trmse: 607.387729981954\n",
      "\t\t\tmae: 433.3881419429569\n"
     ]
    }
   ],
   "source": [
    "for data_name, model in best_models.items():\n",
    "    print(data_name)\n",
    "    for model_name, data in model.items():\n",
    "        print(f\"\\t{model_name}\")\n",
    "        for key, value in data.items():\n",
    "            if key == \"model\":\n",
    "                # print(f\"\\t{key}: {value.explainParams()}\")\n",
    "                print(f\"\\t\\t{key}: {value}\")\n",
    "            else:\n",
    "                print(f\"\\t\\t{key}:\")\n",
    "                for score_name, score_value in value.items():\n",
    "                    print(f\"\\t\\t\\t{score_name}: {score_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1ea97115c56618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:27:47.374296Z",
     "start_time": "2024-05-23T22:27:47.370535Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
